{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Sampling\n",
    "\n",
    "To accelerate development, we'll create a smaller, balanced sample from the full 1.6 million tweet dataset. We will sample 100,000 negative and 100,000 positive tweets to form a new 200,000-tweet dataset. This ensures that our model trains on a representative and manageable subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded successfully. Shape: (1600000, 6)\n",
      "Created and saved a balanced sample of 200000 tweets.\n",
      "Target distribution in the sample:\n",
      "target\n",
      "4    100000\n",
      "0    100000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define paths and column names\n",
    "DATA_PATH = '../data/training.1600000.processed.noemoticon.csv'\n",
    "SAMPLED_DATA_PATH = '../data/sentiment140_sampled_200k.csv'\n",
    "COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the full dataset\n",
    "try:\n",
    "    df_full = pd.read_csv(DATA_PATH, encoding='latin-1', header=None, names=COLUMNS)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {DATA_PATH} was not found.\")\n",
    "    print(\"Please ensure the training data is correctly placed in the 'data' directory.\")\n",
    "else:\n",
    "    print(f\"Full dataset loaded successfully. Shape: {df_full.shape}\")\n",
    "\n",
    "    # Separate by sentiment\n",
    "    df_negative = df_full[df_full['target'] == 0]\n",
    "    df_positive = df_full[df_full['target'] == 4]\n",
    "\n",
    "    # Create balanced samples\n",
    "    df_negative_sampled = df_negative.sample(n=100000, random_state=42)\n",
    "    df_positive_sampled = df_positive.sample(n=100000, random_state=42)\n",
    "\n",
    "    # Concatenate and shuffle the samples\n",
    "    df_sampled = pd.concat([df_negative_sampled, df_positive_sampled])\n",
    "    df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Save the sampled dataset\n",
    "    df_sampled.to_csv(SAMPLED_DATA_PATH, index=False)\n",
    "\n",
    "    print(f\"Created and saved a balanced sample of {df_sampled.shape[0]} tweets.\")\n",
    "    print(\"Target distribution in the sample:\")\n",
    "    print(df_sampled['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load and Verify Sampled Dataset\n",
    "\n",
    "Now, we load the newly created sampled dataset to proceed with the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset loaded. Shape: (200000, 6)\n",
      "Target distribution:\n",
      "target\n",
      "4    100000\n",
      "0    100000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1793657674</td>\n",
       "      <td>Thu May 14 03:31:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marita_holm</td>\n",
       "      <td>Looks like the sun finally located Trondheim ;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1971008586</td>\n",
       "      <td>Sat May 30 05:56:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>addicthim</td>\n",
       "      <td>A long weekend begins. The sun is shining and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1881031249</td>\n",
       "      <td>Fri May 22 03:21:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>susietech</td>\n",
       "      <td>to the beach we go! hope it stays nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1795951084</td>\n",
       "      <td>Thu May 14 08:38:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KatieBlockley</td>\n",
       "      <td>@JBFutureboy I missed it  busted need to do a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1978767005</td>\n",
       "      <td>Sun May 31 00:23:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Camila_love</td>\n",
       "      <td>Why I can't change my background image??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       4  1793657674  Thu May 14 03:31:43 PDT 2009  NO_QUERY    marita_holm   \n",
       "1       0  1971008586  Sat May 30 05:56:45 PDT 2009  NO_QUERY      addicthim   \n",
       "2       4  1881031249  Fri May 22 03:21:42 PDT 2009  NO_QUERY      susietech   \n",
       "3       0  1795951084  Thu May 14 08:38:29 PDT 2009  NO_QUERY  KatieBlockley   \n",
       "4       0  1978767005  Sun May 31 00:23:54 PDT 2009  NO_QUERY    Camila_love   \n",
       "\n",
       "                                                text  \n",
       "0  Looks like the sun finally located Trondheim ;...  \n",
       "1  A long weekend begins. The sun is shining and ...  \n",
       "2         to the beach we go! hope it stays nice...   \n",
       "3  @JBFutureboy I missed it  busted need to do a ...  \n",
       "4          Why I can't change my background image??   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load the sampled dataset\n",
    "df_sampled = pd.read_csv(SAMPLED_DATA_PATH)\n",
    "\n",
    "print(f\"Sampled dataset loaded. Shape: {df_sampled.shape}\")\n",
    "print(\"Target distribution:\")\n",
    "print(df_sampled['target'].value_counts())\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Text Cleaning\n",
    "\n",
    "We will now apply advanced text cleaning techniques to handle the specific characteristics of Twitter data. This includes removing URLs, user mentions, numbers, and normalizing elongated words, while preserving potentially useful information like hashtag text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after advanced cleaning:\n",
      "                                                text  \\\n",
      "0  Looks like the sun finally located Trondheim ;...   \n",
      "1  A long weekend begins. The sun is shining and ...   \n",
      "2         to the beach we go! hope it stays nice...    \n",
      "3  @JBFutureboy I missed it  busted need to do a ...   \n",
      "4          Why I can't change my background image??    \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  looks like the sun finally located trondheim h...  \n",
      "1  a long weekend begins the sun is shining and i...  \n",
      "2              to the beach we go hope it stays nice  \n",
      "3  i missed it busted need to do a reunion tour t...  \n",
      "4              why i cant change my background image  \n"
     ]
    }
   ],
   "source": [
    "def clean_tweet(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove '#' symbol, keeping the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and special characters (keeping spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Normalize elongated words (e.g., 'loooove' -> 'love')\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    # Remove extra spaces and strip leading/trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_sampled['cleaned_text'] = df_sampled['text'].apply(clean_tweet)\n",
    "\n",
    "print(\"Text after advanced cleaning:\")\n",
    "print(df_sampled[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Splitting\n",
    "\n",
    "We will split the cleaned and sampled dataset into training, validation, and test sets. This ensures that our model is trained on one subset of data, tuned on another, and evaluated on a completely unseen third subset to provide an unbiased assessment of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (160000,), (160000,)\n",
      "Validation set shape: (20000,), (20000,)\n",
      "Test set shape: (20000,), (20000,)\n",
      "\n",
      "Training target distribution:\n",
      "target\n",
      "0    0.5\n",
      "4    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation target distribution:\n",
      "target\n",
      "4    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test target distribution:\n",
      "target\n",
      "0    0.5\n",
      "4    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_sampled['cleaned_text']\n",
    "y = df_sampled['target']\n",
    "\n",
    "# Split into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split temporary set into validation and test sets (50% validation, 50% test from temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "print(\"\\nTraining target distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nValidation target distribution:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "print(\"\\nTest target distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 BERT-Specific Tokenization\n",
    "\n",
    "We will tokenize the cleaned text data using a pre-trained BERT tokenizer from the Hugging Face `transformers` library. This process converts text into numerical input IDs and attention masks, which are required for BERT-based models. We will use `vinai/bertweet-base` as it is pre-trained on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Train input_ids shape: torch.Size([160000, 128])\n",
      "Train attention_mask shape: torch.Size([160000, 128])\n",
      "Validation input_ids shape: torch.Size([20000, 128])\n",
      "Validation attention_mask shape: torch.Size([20000, 128])\n",
      "Test input_ids shape: torch.Size([20000, 128])\n",
      "Test attention_mask shape: torch.Size([20000, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = 'vinai/bertweet-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "val_encodings = tokenize_function(X_val.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "print(\"Tokenization complete.\")\n",
    "print(f\"Train input_ids shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Train attention_mask shape: {train_encodings['attention_mask'].shape}\")\n",
    "print(f\"Validation input_ids shape: {val_encodings['input_ids'].shape}\")\n",
    "print(f\"Validation attention_mask shape: {val_encodings['attention_mask'].shape}\")\n",
    "print(f\"Test input_ids shape: {test_encodings['input_ids'].shape}\")\n",
    "print(f\"Test attention_mask shape: {test_encodings['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "We will create a custom PyTorch Dataset class to handle the tokenized text and labels. This allows us to efficiently load and batch data during model training and evaluation. We will then create DataLoader instances for the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Datasets and DataLoaders created.\n",
      "Train loader: 10000 batches of size 16\n",
      "Validation loader: 1250 batches of size 16\n",
      "Test loader: 1250 batches of size 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TweetDataset(train_encodings, y_train_np)\n",
    "val_dataset = TweetDataset(val_encodings, y_val_np)\n",
    "test_dataset = TweetDataset(test_encodings, y_test_np)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"PyTorch Datasets and DataLoaders created.\")\n",
    "print(f\"Train loader: {len(train_loader)} batches of size 16\")\n",
    "print(f\"Validation loader: {len(val_loader)} batches of size 16\")\n",
    "print(f\"Test loader: {len(test_loader)} batches of size 16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
