{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase, we will fine-tune a pre-trained BERT model for our sentiment analysis task. We will use `vinai/bertweet-base`, a model specifically pre-trained on a large corpus of English tweets, making it highly suitable for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Preparation\n",
    "\n",
    "To make this notebook self-contained, we first need to load and prepare the data. This involves repeating the key steps from the `3_Data_Preparation` notebook: loading the sampled data, cleaning the text, splitting it into sets, tokenizing it for BERT, and creating PyTorch DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found. Using Apple Silicon GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete. DataLoaders are ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define path\n",
    "SAMPLED_DATA_PATH = '../data/sentiment140_sampled_200k.csv'\n",
    "\n",
    "# --- Global Model and Device Setup ---\n",
    "MODEL_NAME = 'vinai/bertweet-base'\n",
    "\n",
    "# -- Hardware Check --\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found. Using Apple Silicon GPU.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found. Using NVIDIA GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "# Load the sampled dataset\n",
    "df_sampled = pd.read_csv(SAMPLED_DATA_PATH)\n",
    "\n",
    "# --- Text Cleaning ---\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_sampled['cleaned_text'] = df_sampled['text'].apply(clean_tweet)\n",
    "\n",
    "# --- Data Splitting ---\n",
    "X = df_sampled['cleaned_text'].astype(str) # Ensure text is string\n",
    "y = df_sampled['target'].apply(lambda x: 1 if x == 4 else 0) # Map target to 0 and 1\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# --- Tokenization ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "val_encodings = tokenize_function(X_val.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "# --- PyTorch Datasets and DataLoaders ---\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, y_train.to_numpy())\n",
    "val_dataset = TweetDataset(val_encodings, y_val.to_numpy())\n",
    "test_dataset = TweetDataset(test_encodings, y_test.to_numpy())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Data preparation complete. DataLoaders are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training and Evaluation Functions\n",
    "\n",
    "Here we define the core functions for training and evaluating our PyTorch models. \n",
    "- **`train_epoch`**: Handles one full pass over the training data, including the forward pass, loss calculation, backpropagation, and weight updates. It's compatible with mixed-precision training.\n",
    "- **`eval_model`**: Handles the evaluation on a dataset (e.g., validation or test set), calculating accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scaler):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device.type, enabled=device.type=='cuda' or device.type=='mps'):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.autocast(device_type=device.type, enabled=device.type=='cuda' or device.type=='mps'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu())\n",
    "            actual_labels.extend(labels.cpu())\n",
    "            \n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    f1 = f1_score(actual_labels, predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Classical Baseline Model\n",
    "\n",
    "Before diving into complex transformer models, it's crucial to establish a performance baseline with a simple, classical machine learning model. This gives us a benchmark to measure our transformer against and provides a point of comparison for explainability.\n",
    "\n",
    "We will use a **TF-IDF (Term Frequency-Inverse Document Frequency)** vectorizer to convert the text into numerical features, followed by a **Logistic Regression** classifier. This is a strong and highly interpretable baseline for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model Performance ---\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.78      0.79     10000\n",
      "    Positive       0.79      0.81      0.80     10000\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.79      0.79      0.79     20000\n",
      "weighted avg       0.79      0.79      0.79     20000\n",
      "\n",
      "\n",
      "Baseline model and TF-IDF vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# 1. Create and train the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# 2. Train the Logistic Regression model\n",
    "baseline_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Evaluate the baseline model\n",
    "y_pred_baseline = baseline_model.predict(X_test_tfidf)\n",
    "print(\"--- Baseline Model Performance ---\\n\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# 4. Save the model and vectorizer for later use in Notebook 5\n",
    "joblib.dump(baseline_model, '../models/baseline_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, '../models/tfidf_vectorizer.pkl')\n",
    "print(\"\\nBaseline model and TF-IDF vectorizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transformer Model - Hyperparameter Tuning\n",
    "\n",
    "Now we turn to our transformer model. Instead of using default hyperparameters, we will conduct a systematic search to find the optimal configuration for our specific dataset. This ensures our final model is performing at its best and makes our results more robust.\n",
    "\n",
    "**Strategy:**\n",
    "1.  **Tune on a Subset:** To make this process fast, we will perform the search on a small, stratified subset (20,000 samples) of our training data.\n",
    "2.  **Randomized Search:** We will test a fixed number of random combinations of learning rates and epochs. This is more efficient than testing every single combination (Grid Search).\n",
    "3.  **Evaluate:** Each combination will be evaluated based on its F1-score on the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a tuning subset with 20000 samples.\n",
      "\n",
      "Starting hyperparameter tuning...\n",
      "\n",
      "--- Combination 1/5 ---\n",
      "Testing: Learning Rate=3e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/665624656.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Val Accuracy=50.00%, Val F1=0.3333\n",
      "\n",
      "--- Combination 2/5 ---\n",
      "Testing: Learning Rate=2e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/665624656.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Val Accuracy=85.01%, Val F1=0.8501\n",
      "\n",
      "--- Combination 3/5 ---\n",
      "Testing: Learning Rate=1e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/665624656.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Val Accuracy=85.74%, Val F1=0.8574\n",
      "\n",
      "--- Combination 4/5 ---\n",
      "Testing: Learning Rate=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/665624656.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Val Accuracy=85.31%, Val F1=0.8529\n",
      "\n",
      "--- Combination 5/5 ---\n",
      "Testing: Learning Rate=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/665624656.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Val Accuracy=83.90%, Val F1=0.8381\n",
      "\n",
      "--- Tuning Complete ---\n",
      "Best F1 Score on Validation Set: 0.8574\n",
      "Best Hyperparameters: {'learning_rate': 1e-05, 'epochs': 3}\n",
      "Best hyperparameters saved to ../models/best_params.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json # Import json for saving/loading hyperparameters\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# -- 1. Create a smaller subset for tuning --\n",
    "# We take 20k samples from the training set for faster iteration.\n",
    "X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, train_size=20000, random_state=42, stratify=y_train)\n",
    "\n",
    "subset_encodings = tokenize_function(X_train_subset.tolist())\n",
    "subset_dataset = TweetDataset(subset_encodings, y_train_subset.to_numpy())\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Created a tuning subset with {len(X_train_subset)} samples.\")\n",
    "\n",
    "# -- 2. Define the search space --\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "    'epochs': [2, 3, 4]\n",
    "}\n",
    "num_combinations = 5 # Number of random combinations to test\n",
    "\n",
    "# -- 3. Run the Randomized Search --\n",
    "best_f1 = -1\n",
    "best_params = {}\n",
    "tuning_results = []\n",
    "\n",
    "print(\"\\nStarting hyperparameter tuning...\")\n",
    "for i in range(num_combinations):\n",
    "    # Randomly sample parameters\n",
    "    lr = random.choice(param_grid['learning_rate'])\n",
    "    epochs = random.choice(param_grid['epochs'])\n",
    "    \n",
    "    print(f\"\\n--- Combination {i+1}/{num_combinations} ---\")\n",
    "    print(f\"Testing: Learning Rate={lr}, Epochs={epochs}\")\n",
    "    \n",
    "    # Instantiate a fresh model for each run\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Training loop for this combination\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = train_epoch(model, subset_loader, optimizer, device, scaler)\n",
    "    \n",
    "    # Evaluate on the full validation set\n",
    "    val_accuracy, val_f1 = eval_model(model, val_loader, device)\n",
    "    print(f\"Result: Val Accuracy={val_accuracy*100:.2f}%, Val F1={val_f1:.4f}\")\n",
    "    \n",
    "    tuning_results.append({'lr': lr, 'epochs': epochs, 'f1': val_f1})\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_params = {'learning_rate': lr, 'epochs': epochs}\n",
    "\n",
    "print(\"\\n--- Tuning Complete ---\")\n",
    "print(f\"Best F1 Score on Validation Set: {best_f1:.4f}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Save best_params to a JSON file\n",
    "with open('../models/best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)\n",
    "print(\"Best hyperparameters saved to ../models/best_params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Transformer Model - Final Training\n",
    "\n",
    "Now that we have identified the optimal hyperparameters from our search, we will use them to train our final model on the **full training dataset**. This ensures our model is both well-configured and learns from all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Model Training ---\n",
      "Loaded optimal parameters: {'learning_rate': 1e-05, 'epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/7h/_b0zd4l16hl6hpf99fwxd_s40000gn/T/ipykernel_30815/3919599999.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/Users/skress/Documents/DBU/Machine_Learning/Studienarbeit_Praxis/venv_Machine_Learning/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 51m 56s\n",
      "\tTrain Loss: 0.339\n",
      "\tVal. Accuracy: 87.11%\n",
      "\tVal. F1 Score: 0.871\n",
      "Epoch: 02 | Time: 165m 57s\n",
      "\tTrain Loss: 0.276\n",
      "\tVal. Accuracy: 86.94%\n",
      "\tVal. F1 Score: 0.869\n",
      "Epoch: 03 | Time: 135m 5s\n",
      "\tTrain Loss: 0.220\n",
      "\tVal. Accuracy: 86.42%\n",
      "\tVal. F1 Score: 0.864\n",
      "\n",
      "Final fine-tuned model saved to '../models/bertweet_sentiment_finetuned'\n"
     ]
    }
   ],
   "source": [
    "import json # Import json for saving/loading hyperparameters\n",
    "\n",
    "print(\"--- Starting Final Model Training ---\")\n",
    "\n",
    "# Load best_params from the JSON file\n",
    "try:\n",
    "    with open('../models/best_params.json', 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    print(f\"Loaded optimal parameters: {best_params}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: best_params.json not found. Please run the Hyperparameter Tuning step (4.3) first.\")\n",
    "    # Fallback to default if file not found, or raise an error\n",
    "    best_params = {'learning_rate': 2e-5, 'epochs': 3} # Default values\n",
    "    print(f\"Using default parameters: {best_params}\")\n",
    "\n",
    "# 1. Instantiate the final model\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "# 2. Set up optimizer and scaler with the best learning rate\n",
    "optimizer = AdamW(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# 3. Train on the full training data for the optimal number of epochs\n",
    "for epoch in range(best_params['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use the full 'train_loader' this time\n",
    "    avg_train_loss = train_epoch(final_model, train_loader, optimizer, device, scaler)\n",
    "    val_accuracy, val_f1 = eval_model(final_model, val_loader, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\\tTrain Loss: {avg_train_loss:.3f}')\n",
    "    print(f'\\tVal. Accuracy: {val_accuracy*100:.2f}%')\n",
    "    print(f'\\tVal. F1 Score: {val_f1:.3f}')\n",
    "\n",
    "# 4. Save the final, tuned model\n",
    "final_model.save_pretrained('../models/bertweet_sentiment_finetuned')\n",
    "tokenizer.save_pretrained('../models/bertweet_sentiment_finetuned')\n",
    "print(\"\\nFinal fine-tuned model saved to '../models/bertweet_sentiment_finetuned'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
