{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87a5082",
   "metadata": {},
   "source": [
    "# 6 Deployment — Zusammenfassung & Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04401ed5",
   "metadata": {},
   "source": [
    "## 6.1 Allgemeine Erkenntnisse & Diskussion\n",
    "Die Evaluation aus Notebook 5 zeigt, dass SHAP und Captum beide valide Erklärungen liefern, sich jedoch in Qualität, Robustheit und Geschwindigkeit deutlich unterscheiden. Der folgende Überblick verdichtet die wichtigsten Resultate und bildet die Grundlage für produktseitige Entscheidungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a1389",
   "metadata": {},
   "source": [
    "### 6.1.1 Hypothesenkonsolidierung\n",
    "| Hypothese | Ergebnis | Evidenz (Notebook 5) |\n",
    "| --- | --- | --- |\n",
    "| H1: Token-Übereinstimmung | *Teilweise bestätigt* | Top-5-Überlappung Ø 0,54; FN ≈ 0,57, FP ≈ 0,46 |\n",
    "| H2: Faithfulness | *Bestätigt für SHAP* | SHAP AUC-Ratio ≥ 1,5 in 73 % der Fälle; Captum 49 % |\n",
    "| H3: Robustheit | *Bestätigt für SHAP, eingeschränkt für Captum* | SHAP ρ ≈ 0,75 stabil, Captum ρ ≈ 0,44 (Füllwörter kritisch) |\n",
    "| H4: Rechenaufwand | *Bestätigt (Captum schneller)* | SHAP ≈ 22,9 s vs. Captum ≈ 10,0 s (≈ 2,3× schneller) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24587340",
   "metadata": {},
   "source": [
    "### 6.1.2 Kernaussagen für Stakeholder\n",
    "- **Qualität vs. Kosten:** SHAP liefert konsistentere und robustere Erklärungen, kostet jedoch mehr Laufzeit. Captum eignet sich für schnelle Erstanalysen.\n",
    "- **Fehlerfokus:** Divergenzen treten besonders bei False Positives auf. Eine manuelle Zweitprüfung dieser Fälle ist empfehlenswert.\n",
    "- **XAI-Design:** Eine hybride Strategie (Captum für Streaming, SHAP für Audits) verbindet Geschwindigkeit und Aussagekraft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d6560",
   "metadata": {},
   "source": [
    "## 6.2 Überlegungen zum Deployment\n",
    "Ziel ist ein reproduzierbarer, wartbarer Inferenz-Stack, der sowohl Modellvorhersagen als auch erklärende Artefakte bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e510e99",
   "metadata": {},
   "source": [
    "### 6.2.1 Deployment-Ziele\n",
    "- **Hauptziel:** Bereitstellung eines Sentiment-Services mit optionalen XAI-Erklärungen (SHAP für Audits, Captum für Online-Erklärbarkeit).\n",
    "- **Sekundärziel:** Sicherstellung, dass Ergebnisse aus Notebook 5 reproduzierbar bleiben (Versionierung von Code, Daten, Modellen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289982e",
   "metadata": {},
   "source": [
    "### 6.2.2 Technische Anforderungen\n",
    "- **Artefakte:** Feinabgestimmtes `vinai/bertweet-base`, Tokenizer, PyTorch-Lightning-Checkpoint, Konfigurationsdateien.\n",
    "- **Laufzeitumgebung:** Python 3.10+, PyTorch ≥ 2.0, Transformers ≥ 4.57, Captum, SHAP, PyArrow (für Caching), optional ONNX Runtime für Beschleunigung.\n",
    "- **Hardware:** CPU-Only Betrieb möglich (≈ 100 ms inference pro Tweet), GPU/MPS zur Beschleunigung von Batches und SHAP-Analysen empfehlenswert.\n",
    "- **Konfigurierbarkeit:** Environment Variablen für Modellpfade, Batchgrößen, Wahl der Erklärmethode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83701117",
   "metadata": {},
   "source": [
    "### 6.2.3 Serving-Architektur\n",
    "1. **Preprocessing-Service:** Repliziert die Cleaning-Logik aus Notebook 3 (Regex, Normalisierung).\n",
    "2. **Inference-Service:** Lädt das fine-tuned Modell, bietet REST/gRPC-Endpunkte für Sentiment-Predictions.\n",
    "3. **Explainability-Service:**\n",
    "   - *Captum-Pfad:* Schnell, liefert Top-Tokens via LayerIntegratedGradients.\n",
    "   - *SHAP-Pfad:* Asynchroner Job (Celery/Kafka) für tiefgehende Analysen; Ergebnisse werden gecached.\n",
    "4. **Storage & Caching:**\n",
    "   - Redis / SQLite für kurzfristige Ergebnisse.\n",
    "   - Langfristige Speicherung in S3/Parquet (bereits vorbereitet in Notebook 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05479f",
   "metadata": {},
   "source": [
    "### 6.2.4 Betriebsprozesse & Überwachung\n",
    "- **Monitoring:**\n",
    "  - Metriken: Latenz (p95), Fehlerraten, Anteil der Erklärungsanfragen, Drift-Indikatoren auf Token-Ebene.\n",
    "  - Logging: Speicherung von (Text, Prediction, Explanation-ID) für Audits (DSGVO-konform).\n",
    "- **Alerting:** Schwellen für Latenz > 500 ms oder wiederholte Ausfälle beim SHAP-Worker.\n",
    "- **Governance:** Versionierung über DVC/MLflow; jedes Deployment enthält Modell-, Daten- und Evaluations-Hash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d90b62",
   "metadata": {},
   "source": [
    "### 6.2.5 Produktionsreife Checkliste\n",
    "1. **Reproduzierbares Build:** Dockerfile mit Mehrstufen-Build, automatisierte Tests (PyTest, Smoke Tests).\n",
    "2. **CI/CD-Pipeline:** GitHub Actions oder Azure DevOps; Stages: Lint → Unit Tests → Modell-Drift-Check → Deployment (staging → prod).\n",
    "3. **Security:** Secrets-Management, Rate Limiting, Eingabevalidierung.\n",
    "4. **UX/API:** Konsistentes Schema (JSON) für Antwort inkl. Scores, Token-Attributionslisten, optionaler Visualisierungshyperlink.\n",
    "5. **Rollbacks:** Canary-Deployments, automatisiertes Zurücksetzen bei Regressionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a5187",
   "metadata": {},
   "source": [
    "## 6.3 Einschränkungen & Nächste Schritte\n",
    "Eine reflektierte Betrachtung der Projektgrenzen hilft, Risiken beim Deployment zu adressieren und die Roadmap für weitere Iterationen festzulegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b48b1",
   "metadata": {},
   "source": [
    "### 6.3.1 Bekannte Einschränkungen\n",
    "- **Datenbasis:** Fokus auf englische Tweets aus 2009; Generalisierbarkeit auf aktuelle Plattformen oder andere Sprachen nicht geprüft.\n",
    "- **Attributionsstreuung:** Captum-Sensitivität bei Perturbationen deutet auf mögliche Instabilität im Live-Betrieb.\n",
    "- **Compute-Kosten:** SHAP-Aufwand skaliert schlecht; Produktivbetrieb benötigt dedizierte Ressourcen oder Sampling-Strategien.\n",
    "- **Bias-Risiko:** Keine Fairness- oder Bias-Audits durchgeführt; vor Deployment erforderlich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e9721",
   "metadata": {},
   "source": [
    "### 6.3.2 Empfohlene nächste Schritte\n",
    "1. **Erweiterte Evaluation:** Größere Stichprobe (> 200 Tweets), zusätzliche Perturbationen (Code-Switching, Emojis).\n",
    "2. **User Research:** Interviews mit Analyst:innen zur Verständlichkeit der Erklärungen, Anpassung des UI basierend auf Feedback.\n",
    "3. **Hybrid-Explainability:** Implementierung eines zweistufigen Systems (Captum online, SHAP offline) und A/B-Test gegen rein Captum-basierten Flow.\n",
    "4. **Model Monitoring MVP:** Aufbau eines Monitoring-Dashboards (Grafana/Prometheus) mit Echtzeit-Alarmierung.\n",
    "5. **Compliance-Check:** Datenschutz-Freigabe, Dokumentation für Audit (Modellkarten, Datenblätter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288865",
   "metadata": {},
   "source": [
    "## 6.4 Roadmap zur Produktionsreife\n",
    "- **Kurzfristig (0–1 Monat):** Dockerisieren, CI/CD aufsetzen, Captum-Only Service ausrollen, Monitoring-Basics implementieren.\n",
    "- **Mittelfristig (1–3 Monate):** SHAP-Offloading-Service etablieren, UI-Komponenten bauen, Fairness-Checks durchführen.\n",
    "- **Langfristig (>3 Monate):** Erweiterung auf Multilingualität, kontinuierliches Retraining mit Feedback-Schleifen, Integration in CRM/Support-Systeme."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
