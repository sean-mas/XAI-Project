{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87a5082",
   "metadata": {},
   "source": [
    "# 6 Deployment — Zusammenfassung & Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04401ed5",
   "metadata": {},
   "source": [
    "## 6.1 Allgemeine Erkenntnisse & Diskussion\n",
    "Die Evaluation aus Notebook 5 zeigt, dass SHAP und Captum beide valide Erklärungen liefern, sich jedoch in Qualität, Robustheit und Geschwindigkeit deutlich unterscheiden. Der folgende Überblick verdichtet die wichtigsten Resultate und bildet die Grundlage für produktseitige Entscheidungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a1389",
   "metadata": {},
   "source": [
    "### 6.1.1 Hypothesenkonsolidierung\n",
    "| Hypothese | Ergebnis | Evidenz (Notebook 5) |\n",
    "| --- | --- | --- |\n",
    "| H1: Token-Übereinstimmung | *Teilweise bestätigt* | Top-5-Überlappung Ø 0,54; FN ≈ 0,57, FP ≈ 0,46 |\n",
    "| H2: Faithfulness | *Bestätigt für SHAP* | SHAP AUC-Ratio ≥ 1,5 in 73 % der Fälle; Captum 49 % |\n",
    "| H3: Robustheit | *Bestätigt für SHAP, eingeschränkt für Captum* | SHAP ρ ≈ 0,75 stabil, Captum ρ ≈ 0,44 (Füllwörter kritisch) |\n",
    "| H4: Rechenaufwand | *Bestätigt (Captum schneller)* | SHAP ≈ 22,9 s vs. Captum ≈ 10,0 s (≈ 2,3× schneller) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24587340",
   "metadata": {},
   "source": [
    "### 6.1.2 Kernaussagen für Stakeholder\n",
    "- **Qualität vs. Kosten:** SHAP liefert konsistentere und robustere Erklärungen, kostet jedoch mehr Laufzeit. Captum eignet sich für schnelle Erstanalysen.\n",
    "- **Fehlerfokus:** Divergenzen treten besonders bei False Positives auf. Eine manuelle Zweitprüfung dieser Fälle ist empfehlenswert.\n",
    "- **XAI-Design:** Eine hybride Strategie (Captum für Streaming, SHAP für Audits) verbindet Geschwindigkeit und Aussagekraft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d6560",
   "metadata": {},
   "source": [
    "## 6.2 Überlegungen zum Deployment\n",
    "Ziel ist ein reproduzierbarer, wartbarer Inferenz-Stack, der sowohl Modellvorhersagen als auch erklärende Artefakte bereitstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e510e99",
   "metadata": {},
   "source": [
    "### 6.2.1 Deployment-Ziele\n",
    "- **Hauptziel:** Bereitstellung eines Sentiment-Services mit optionalen XAI-Erklärungen (SHAP für Audits, Captum für Online-Erklärbarkeit).\n",
    "- **Sekundärziel:** Sicherstellung, dass Ergebnisse aus Notebook 5 reproduzierbar bleiben (Versionierung von Code, Daten, Modellen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289982e",
   "metadata": {},
   "source": [
    "### 6.2.2 Technische Anforderungen\n",
    "- **Artefakte:** Feinabgestimmtes `vinai/bertweet-base`, Tokenizer, PyTorch-Lightning-Checkpoint, Konfigurationsdateien.\n",
    "- **Laufzeitumgebung:** Python 3.10+, PyTorch ≥ 2.0, Transformers ≥ 4.57, Captum, SHAP, PyArrow (für Caching), optional ONNX Runtime für Beschleunigung.\n",
    "- **Hardware:** CPU-Only Betrieb möglich (≈ 100 ms inference pro Tweet), GPU/MPS zur Beschleunigung von Batches und SHAP-Analysen empfehlenswert.\n",
    "- **Konfigurierbarkeit:** Environment Variablen für Modellpfade, Batchgrößen, Wahl der Erklärmethode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83701117",
   "metadata": {},
   "source": [
    "### 6.2.3 Serving-Architektur\n",
    "1. **Preprocessing-Service:** Repliziert die Cleaning-Logik aus Notebook 3 (Regex, Normalisierung).\n",
    "2. **Inference-Service:** Lädt das fine-tuned Modell, bietet REST/gRPC-Endpunkte für Sentiment-Predictions.\n",
    "3. **Explainability-Service:**\n",
    "   - *Captum-Pfad:* Schnell, liefert Top-Tokens via LayerIntegratedGradients.\n",
    "   - *SHAP-Pfad:* Asynchroner Job (Celery/Kafka) für tiefgehende Analysen; Ergebnisse werden gecached.\n",
    "4. **Storage & Caching:**\n",
    "   - Redis / SQLite für kurzfristige Ergebnisse.\n",
    "   - Langfristige Speicherung in S3/Parquet (bereits vorbereitet in Notebook 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05479f",
   "metadata": {},
   "source": [
    "### 6.2.4 Betriebsprozesse & Überwachung\n",
    "- **Monitoring:**\n",
    "  - Metriken: Latenz (p95), Fehlerraten, Anteil der Erklärungsanfragen, Drift-Indikatoren auf Token-Ebene.\n",
    "  - Logging: Speicherung von (Text, Prediction, Explanation-ID) für Audits (DSGVO-konform).\n",
    "- **Alerting:** Schwellen für Latenz > 500 ms oder wiederholte Ausfälle beim SHAP-Worker.\n",
    "- **Governance:** Versionierung über DVC/MLflow; jedes Deployment enthält Modell-, Daten- und Evaluations-Hash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d90b62",
   "metadata": {},
   "source": [
    "### 6.2.5 Produktionsreife Checkliste\n",
    "1. **Reproduzierbares Build:** Dockerfile mit Mehrstufen-Build, automatisierte Tests (PyTest, Smoke Tests).\n",
    "2. **CI/CD-Pipeline:** GitHub Actions oder Azure DevOps; Stages: Lint → Unit Tests → Modell-Drift-Check → Deployment (staging → prod).\n",
    "3. **Security:** Secrets-Management, Rate Limiting, Eingabevalidierung.\n",
    "4. **UX/API:** Konsistentes Schema (JSON) für Antwort inkl. Scores, Token-Attributionslisten, optionaler Visualisierungshyperlink.\n",
    "5. **Rollbacks:** Canary-Deployments, automatisiertes Zurücksetzen bei Regressionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a5187",
   "metadata": {},
   "source": [
    "## 6.3 Einschränkungen & Nächste Schritte\n",
    "Eine reflektierte Betrachtung der Projektgrenzen hilft, Risiken beim Deployment zu adressieren und die Roadmap für weitere Iterationen festzulegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b48b1",
   "metadata": {},
   "source": [
    "### 6.3.1 Bekannte Einschränkungen\n",
    "- **Datenbasis:** Fokus auf englische Tweets aus 2009; Generalisierbarkeit auf aktuelle Plattformen oder andere Sprachen nicht geprüft.\n",
    "- **Attributionsstreuung:** Captum-Sensitivität bei Perturbationen deutet auf mögliche Instabilität im Live-Betrieb.\n",
    "- **Compute-Kosten:** SHAP-Aufwand skaliert schlecht; Produktivbetrieb benötigt dedizierte Ressourcen oder Sampling-Strategien.\n",
    "- **Bias-Risiko:** Keine Fairness- oder Bias-Audits durchgeführt; vor Deployment erforderlich."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e9721",
   "metadata": {},
   "source": [
    "### 6.3.2 Empfohlene nächste Schritte\n",
    "1. **Erweiterte Evaluation:** Größere Stichprobe (> 200 Tweets), zusätzliche Perturbationen (Code-Switching, Emojis).\n",
    "2. **User Research:** Interviews mit Analyst:innen zur Verständlichkeit der Erklärungen, Anpassung des UI basierend auf Feedback.\n",
    "3. **Hybrid-Explainability:** Implementierung eines zweistufigen Systems (Captum online, SHAP offline) und A/B-Test gegen rein Captum-basierten Flow.\n",
    "4. **Model Monitoring MVP:** Aufbau eines Monitoring-Dashboards (Grafana/Prometheus) mit Echtzeit-Alarmierung.\n",
    "5. **Compliance-Check:** Datenschutz-Freigabe, Dokumentation für Audit (Modellkarten, Datenblätter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288865",
   "metadata": {},
   "source": [
    "## 6.4 Roadmap zur Produktionsreife\n",
    "- **Kurzfristig (0–1 Monat):** Dockerisieren, CI/CD aufsetzen, Captum-Only Service ausrollen, Monitoring-Basics implementieren.\n",
    "- **Mittelfristig (1–3 Monate):** SHAP-Offloading-Service etablieren, UI-Komponenten bauen, Fairness-Checks durchführen.\n",
    "- **Langfristig (>3 Monate):** Erweiterung auf Multilingualität, kontinuierliches Retraining mit Feedback-Schleifen, Integration in CRM/Support-Systeme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8e0e7",
   "metadata": {},
   "source": [
    "## 6.5 Production-Ready FastAPI Implementation\n",
    "\n",
    "Below is a complete FastAPI service implementation incorporating all deployment best practices identified in previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Production-Ready FastAPI Sentiment Analysis Service with XAI\n",
    "Save this as: api/main.py\n",
    "Run with: uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, status\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional, Dict, Literal\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "import shap\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prometheus metrics\n",
    "PREDICTION_COUNTER = Counter('predictions_total', 'Total predictions made', ['sentiment'])\n",
    "PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "EXPLANATION_COUNTER = Counter('explanations_total', 'Total explanations generated', ['method'])\n",
    "EXPLANATION_LATENCY = Histogram('explanation_latency_seconds', 'Explanation latency', ['method'])\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"BERTweet Sentiment Analysis API\",\n",
    "    description=\"Production-ready sentiment analysis with SHAP/Captum explainability\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global model variables (loaded at startup)\n",
    "model = None\n",
    "tokenizer = None\n",
    "lig_explainer = None\n",
    "shap_explainer = None\n",
    "device = None\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = Path(\"../models/bertweet_sentiment_finetuned\")\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    \"\"\"Request schema for sentiment prediction\"\"\"\n",
    "    text: str = Field(..., min_length=1, max_length=500, description=\"Tweet text to analyze\")\n",
    "    explain: bool = Field(False, description=\"Whether to include attribution explanations\")\n",
    "    explain_method: Literal[\"captum\", \"shap\", \"both\"] = Field(\"captum\", description=\"Explanation method\")\n",
    "    top_k: int = Field(5, ge=1, le=20, description=\"Number of top attribution tokens to return\")\n",
    "    \n",
    "    @validator('text')\n",
    "    def clean_text(cls, v):\n",
    "        \"\"\"Apply basic text cleaning\"\"\"\n",
    "        v = v.strip()\n",
    "        if not v:\n",
    "            raise ValueError(\"Text cannot be empty after cleaning\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class TokenAttribution(BaseModel):\n",
    "    \"\"\"Token-level attribution score\"\"\"\n",
    "    token: str\n",
    "    attribution: float\n",
    "    position: int\n",
    "\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Response schema for predictions\"\"\"\n",
    "    sentiment: Literal[\"negative\", \"positive\"]\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    probabilities: Dict[str, float]\n",
    "    latency_ms: float\n",
    "    explanations: Optional[Dict[str, List[TokenAttribution]]] = None\n",
    "\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    \"\"\"Replicate cleaning logic from Notebook 3\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load model and explainers at startup\"\"\"\n",
    "    global model, tokenizer, lig_explainer, shap_explainer, device\n",
    "    \n",
    "    logger.info(\"Loading model and tokenizer...\")\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise RuntimeError(f\"Model not found at {MODEL_PATH}\")\n",
    "    \n",
    "    # Device selection\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize Captum explainer\n",
    "    logger.info(\"Initializing Captum LayerIntegratedGradients...\")\n",
    "    lig_explainer = LayerIntegratedGradients(\n",
    "        lambda input_ids, attention_mask: model(input_ids=input_ids, attention_mask=attention_mask).logits,\n",
    "        model.roberta.embeddings\n",
    "    )\n",
    "    \n",
    "    # Initialize SHAP explainer\n",
    "    logger.info(\"Initializing SHAP explainer...\")\n",
    "    def shap_predict_fn(texts):\n",
    "        encodings = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**encodings).logits\n",
    "        return torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    shap_explainer = shap.Explainer(shap_predict_fn, tokenizer)\n",
    "    \n",
    "    logger.info(\"Model and explainers loaded successfully\")\n",
    "\n",
    "\n",
    "@app.get(\"/health\", status_code=status.HTTP_200_OK)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for monitoring\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Model not loaded\"\n",
    "        )\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse, status_code=status.HTTP_200_OK)\n",
    "async def predict_sentiment(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Predict sentiment with optional explainability\n",
    "    \n",
    "    - **text**: Tweet text to analyze (1-500 characters)\n",
    "    - **explain**: Whether to include token attributions\n",
    "    - **explain_method**: Explanation method (captum, shap, or both)\n",
    "    - **top_k**: Number of top attribution tokens to return\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Preprocess text\n",
    "        cleaned_text = clean_tweet(request.text)\n",
    "        if not cleaned_text:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_400_BAD_REQUEST,\n",
    "                detail=\"Text is empty after cleaning\"\n",
    "            )\n",
    "        \n",
    "        # Tokenize and predict\n",
    "        with PREDICTION_LATENCY.time():\n",
    "            encoding = tokenizer(\n",
    "                cleaned_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(**encoding).logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        sentiment_idx = int(probs.argmax())\n",
    "        sentiment = \"positive\" if sentiment_idx == 1 else \"negative\"\n",
    "        confidence = float(probs[sentiment_idx])\n",
    "        \n",
    "        # Update metrics\n",
    "        PREDICTION_COUNTER.labels(sentiment=sentiment).inc()\n",
    "        \n",
    "        # Generate explanations if requested\n",
    "        explanations = None\n",
    "        if request.explain:\n",
    "            explanations = {}\n",
    "            \n",
    "            if request.explain_method in [\"captum\", \"both\"]:\n",
    "                with EXPLANATION_LATENCY.labels(method=\"captum\").time():\n",
    "                    captum_attrs = generate_captum_attributions(\n",
    "                        cleaned_text, encoding, sentiment_idx, request.top_k\n",
    "                    )\n",
    "                    explanations[\"captum\"] = captum_attrs\n",
    "                    EXPLANATION_COUNTER.labels(method=\"captum\").inc()\n",
    "            \n",
    "            if request.explain_method in [\"shap\", \"both\"]:\n",
    "                with EXPLANATION_LATENCY.labels(method=\"shap\").time():\n",
    "                    shap_attrs = generate_shap_attributions(\n",
    "                        cleaned_text, sentiment_idx, request.top_k\n",
    "                    )\n",
    "                    explanations[\"shap\"] = shap_attrs\n",
    "                    EXPLANATION_COUNTER.labels(method=\"shap\").inc()\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            sentiment=sentiment,\n",
    "            confidence=confidence,\n",
    "            probabilities={\"negative\": float(probs[0]), \"positive\": float(probs[1])},\n",
    "            latency_ms=latency_ms,\n",
    "            explanations=explanations\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n",
    "            detail=f\"Prediction failed: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_captum_attributions(text: str, encoding, target_class: int, top_k: int) -> List[TokenAttribution]:\n",
    "    \"\"\"Generate Captum LayerIntegratedGradients attributions\"\"\"\n",
    "    baseline = torch.zeros_like(encoding['input_ids'])\n",
    "    \n",
    "    attributions = lig_explainer.attribute(\n",
    "        inputs=encoding['input_ids'],\n",
    "        baselines=baseline,\n",
    "        additional_forward_args=(encoding['attention_mask'],),\n",
    "        target=target_class,\n",
    "        return_convergence_delta=False\n",
    "    )\n",
    "    \n",
    "    # Sum over embedding dimension\n",
    "    token_attrs = attributions.sum(dim=-1).squeeze().cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze().cpu().tolist())\n",
    "    \n",
    "    # Filter special tokens and get top-k\n",
    "    results = []\n",
    "    for idx, (token, attr) in enumerate(zip(tokens, token_attrs)):\n",
    "        if token not in tokenizer.all_special_tokens:\n",
    "            results.append(TokenAttribution(token=token, attribution=float(attr), position=idx))\n",
    "    \n",
    "    results.sort(key=lambda x: abs(x.attribution), reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "def generate_shap_attributions(text: str, target_class: int, top_k: int) -> List[TokenAttribution]:\n",
    "    \"\"\"Generate SHAP Partition explainer attributions\"\"\"\n",
    "    shap_values = shap_explainer([text])\n",
    "    \n",
    "    # Extract token-level SHAP values for target class\n",
    "    tokens = shap_values.data[0]\n",
    "    values = shap_values.values[0, :, target_class]\n",
    "    \n",
    "    results = []\n",
    "    for idx, (token, value) in enumerate(zip(tokens, values)):\n",
    "        if token.strip():  # Non-empty tokens\n",
    "            results.append(TokenAttribution(token=token, attribution=float(value), position=idx))\n",
    "    \n",
    "    results.sort(key=lambda x: abs(x.attribution), reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "\n",
    "@app.post(\"/batch_predict\", status_code=status.HTTP_200_OK)\n",
    "async def batch_predict(texts: List[str], explain: bool = False):\n",
    "    \"\"\"Batch prediction endpoint for multiple texts\"\"\"\n",
    "    if len(texts) > BATCH_SIZE:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=f\"Batch size exceeds maximum of {BATCH_SIZE}\"\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        request = PredictionRequest(text=text, explain=explain)\n",
    "        result = await predict_sentiment(request)\n",
    "        results.append(result)\n",
    "    \n",
    "    return {\"predictions\": results, \"count\": len(results)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ae7d1",
   "metadata": {},
   "source": [
    "### 6.5.1 API Usage Examples\n",
    "\n",
    "Example requests using curl or Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8737982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple prediction without explanations\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/predict\",\n",
    "    json={\"text\": \"I love this product! Amazing quality!\"}\n",
    ")\n",
    "print(response.json())\n",
    "# Output: {\"sentiment\": \"positive\", \"confidence\": 0.95, \"probabilities\": {...}, \"latency_ms\": 45.2}\n",
    "\n",
    "# Example 2: Prediction with Captum explanations\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/predict\",\n",
    "    json={\n",
    "        \"text\": \"Terrible service, very disappointed\",\n",
    "        \"explain\": True,\n",
    "        \"explain_method\": \"captum\",\n",
    "        \"top_k\": 3\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "# Example 3: Batch prediction\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/batch_predict\",\n",
    "    json={\n",
    "        \"texts\": [\n",
    "            \"Great experience!\",\n",
    "            \"Worst purchase ever\",\n",
    "            \"It's okay, nothing special\"\n",
    "        ],\n",
    "        \"explain\": False\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "# Example 4: Health check\n",
    "response = requests.get(\"http://localhost:8000/health\")\n",
    "print(response.json())\n",
    "# Output: {\"status\": \"healthy\", \"model_loaded\": true, \"device\": \"cpu\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5417edd",
   "metadata": {},
   "source": [
    "### 6.5.2 Deployment Requirements\n",
    "\n",
    "To deploy this API, install additional dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e505d",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Install FastAPI and production dependencies\n",
    "pip install fastapi uvicorn[standard] pydantic prometheus-client\n",
    "\n",
    "# Add to requirements.txt:\n",
    "# fastapi==0.109.0\n",
    "# uvicorn[standard]==0.27.0\n",
    "# pydantic==2.5.0\n",
    "# prometheus-client==0.19.0\n",
    "\n",
    "# Run the API locally\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n",
    "\n",
    "# For production deployment with Gunicorn:\n",
    "gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app --bind 0.0.0.0:8000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
